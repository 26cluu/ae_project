# -*- coding: utf-8 -*-
"""ae project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oyhEKQlLFanWOdRXTEZ3RzlXQiWC9tuQ
"""



import pandas as pd
import numpy as np
import statsmodels.api as sm
import seaborn as sns
from ISLP import load_data
from ISLP.models import (ModelSpec as MS, summarize, poly)
import matplotlib.pyplot as plt
import altair as alt


df = pd.read_csv('car_prices.csv', on_bad_lines = 'warn')

df['color'].unique()

df = df.drop(['model', 'vin', 'trim', 'state', 'interior', 'seller', 'mmr', 'saledate'], axis=1)

df = df.dropna(subset = ["year", "condition", "odometer", "sellingprice", "make", "transmission", "body", "color"])

make_dummies = pd.get_dummies(df['make'], prefix_sep='_', prefix='make')
transmission_dummies = pd.get_dummies(df['transmission'], prefix_sep='_', prefix='transmission')
body_dummies = pd.get_dummies(df['body'], prefix_sep='_', prefix='body')
color_dummies = pd.get_dummies(df['color'], prefix_sep='_', prefix='color')

df = pd.concat([df, make_dummies, transmission_dummies, body_dummies, color_dummies], axis=1)

df.shape

df.columns

features = ['year', 'condition', 'odometer'] + list(make_dummies.columns) + list(transmission_dummies.columns) + list(body_dummies.columns) + list(color_dummies.columns)
spec = MS(features).fit(df)
x = spec.transform(df)
y = df['sellingprice']

model1 = sm.OLS(y, x)
results = model1.fit()

results.rsquared

summarize(results)

column_names = df.columns
df_new = pd.DataFrame(columns = column_names)
df_new = df_new.drop(['sellingprice'], axis=1)
zeros_list = [0] * 160


# predicting based off input data
df_new.loc[0] = [2007, 'Bsfs', 'Sedan', 'automatic', 3.0, 116000, 'gray'] + zeros_list
df_new.loc[0, 'make_BMW'] = 1
df_new.loc[0, 'color_beige'] = 1
df_new.loc[0, 'body_Sedan'] = 1
df_new.loc[0, 'transmission_automatic'] = 1

# prints prediction
new_X = spec.transform(df_new)
results.get_prediction(new_X).predicted_mean

sns.displot(data = df, x = 'condition', col = 'year')

sns.displot(data = df, x = len(df.columns) ,col = 'year')

sns.lineplot(data = df, x = 'year', y = 'condition')

def plot_lines(x_input, y_input):
    fig = plt.figure(figsize = (10, 4))
    sns.lineplot(data = df, x = x_input, y = y_input)
    sns.set(rc={'axes.facecolor': '#0d1118', 'figure.facecolor': '#0d1118', 'axes.labelcolor': 'white', 'text.color': 'white', 'xtick.color': 'white', 'ytick.color': 'white'})
    fig.show()

def plotBarColorGraph():
    color_counts = df['color'].value_counts()

    fig, ax = plt.subplots()

    sns.barplot(x=color_counts.index, y=color_counts.values, ax = ax)

    ax.set_xlabel('Color')
    ax.set_ylabel('Number of Cars')
    ax.set_title('Number of Cars per Color')
    plt.xticks(rotation=45)
    plt.tight_layout()
    fig.show()

def plotAutoManual():
    transmission_counts = df['transmission'].value_counts()
    fig, ax = plt.subplots()

    sns.barplot(x=transmission_counts.index, y=transmission_counts.values, ax = ax)

    ax.set_xlabel('Type of Transmission')
    ax.set_ylabel('Number of Carss')
    ax.set_title('Automatic vs Manual Transmission')
    plt.xticks(rotation=45)
    plt.tight_layout()
    fig.show()

def plotTransmissionCost():
    sns.scatterplot(data=df, x='year', y='sellingprice', hue='transmission', style='transmission')
    plt.xlabel('Year')
    plt.ylabel('Price')
    plt.title('Price Comparison of Manual vs Automatic Cars')
    plt.tight_layout()
    plt.gca().set_aspect('auto')

    min_year = df['year'].min()
    max_year = df['year'].max()
    x_range = max_year - min_year
    plt.xlim(min_year - 0.1 * x_range, max_year + 0.1 * x_range)

    plt.gcf().show()

plot_lines('year', 'condition')

plot_lines('sellingprice', 'condition')

plot_lines('year', 'odometer')

plotBarColorGraph()

plotAutoManual()

plotTransmissionCost()

"""Overall, lots of the data makes a ton of sense. For example, in terms of pure coefficient strength, year, odometer, and condition. Similarly, when someone looks for a car, typically they look for the mileage, the condition it is in, and how old the car is as a starting point. I think this is a good sign since it meant that our model found the same factors to be important as a normal person would when looking at a car. However, there was definitely some very interesting points about the data that showcased certain trends. For example, it was able to get a general understanding of what car companies are considered as higher value brands such as BMW, compared lower value brands such as a honda or toyota. Overal, I found that the most interesting fact was that color has a fairly noticeable impact on price, yet most people when looking at cars don't exaclty consider it to be much of a number mover compared to the usual mileage and years. Another interesting tidbit was that manual and transmission had extremely similar coefficients, indicating that they don't have as much of an effect on value as people may presume. I think this is mainly due to the sheer limited number of manual cars along with their specialty markets in the sports car industry being fairly small, meaning overall only a very small part of the car community cares all that much. Finally, after playing around with the predictions, I found that it was fairly accurated in the more common types of cars like a honda civic in a sweetspot of around 1990 - 2010 for the year. Outside of this range and with more exotic makes and colors, the predictions tend to get very out there. This is probably due to the lack of data for those outer extremes. In summary, the project ended up teaching me a lot about which factors get prioritized over others in the car market, along with general benefits and drawbacks of this type of machine learning. While it was fairly easy to get the ball rolling and set up, it quickly ran into a wall for it to grow further."""